{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Exercise | Requires coding | What to code (if anything) | What should you learn (or practice)    | Done it! |\n",
    "|----------|-----------------|----------------------------|----------------------------------------|----------|\n",
    "| 5.1      | Yes             |    Backpropagation                         | Backprop details                           | No       |\n",
    "| 5.2      | No              |                            | Familiarize with theano                           | No       |\n",
    "| 5.3      |   Yes         |   Forward pass MLP                            | Symbolic forward pass                                | No       |\n",
    "| 5.4      |   No         |        | check TheanoMLP  class                           | No       |\n",
    "| 5.5      |   No         |        | Data in theano (shared variables)                                | No       |\n",
    "| 5.6      |   No         |        | Compare Numpy vs Theano                                | No       |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../lxmls-toolkit')\n",
    "import lxmls\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecxercise 5.1\n",
    "\n",
    "**Start by loading Amazon sentiment corpus used in day 1**\n",
    "\n",
    "- Go to **lxmls/deep\\_learning/mlp.py** look at the class NumpyMLP and the function ** grads( )**. \n",
    "\n",
    "- Complete the code of the NumpyMLP class with the Backpropagation recursion that explained in the guide.\n",
    "\n",
    "- Once you are done. Try different network geometries by increasing the number of layers and layer sizes...\n",
    "\n",
    "\n",
    "#### Important\n",
    "\n",
    "The code is build to accept data as columns. \n",
    "\n",
    "For example a minibatch with 10 examples should have shape `n_features x 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")\n",
    "train_x = scr.train_X.T\n",
    "train_y = scr.train_y[:, 0]\n",
    "test_x = scr.test_X.T\n",
    "test_y = scr.test_y[:, 0]\n",
    "\n",
    "train_x = train_x.astype(\"float32\")\n",
    "train_y = train_y.astype(\"int32\")\n",
    "test_x = test_x.astype(\"float32\")\n",
    "test_y = test_y.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique classes in the data:  [0 1]\n",
      "train shape:  (13989, 1600)\n",
      "test shape:  (13989, 1600)\n"
     ]
    }
   ],
   "source": [
    "print \"unique classes in the data: \", np.unique(train_y)\n",
    "print \"train shape: \", train_x.shape\n",
    "print \"test shape: \", train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network modules\n",
    "import lxmls.deep_learning.mlp as dl\n",
    "import lxmls.deep_learning.sgd as sgd\n",
    "\n",
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "\n",
    "# Instantiate model\n",
    "mlp = dl.NumpyMLP(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers:  2\n"
     ]
    }
   ],
   "source": [
    "print \"number of layers: \", mlp.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 13989), (100, 1), (2, 100), (2, 1)]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in mlp.params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### About mlp.forward \n",
    "\n",
    "the function `mlp.forward()` returns the probability of each of the classes for each example in the minibatch as a column vector. \n",
    "\n",
    "For example, mlp.forward(train_x[:,0:3]) should return \n",
    "\n",
    "```\n",
    "[p(y|x[:,1]), p(y|x[:,2]), p(y|x[:,3])]\n",
    "```\n",
    "\n",
    "which will look like\n",
    "\n",
    "```\n",
    "array([[ 0.71735224,  0.11738626,  0.93179871],\n",
    "       [ 0.28264776,  0.88261374,  0.06820129]])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33014659,  0.53808734,  0.84166145],\n",
       "       [ 0.66985341,  0.46191266,  0.15833855]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(train_x[:,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the gradient shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33014659,  0.53808734,  0.84166145],\n",
       "       [ 0.66985341,  0.46191266,  0.15833855]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(train_x[:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13989)\n",
      "(100, 1)\n",
      "(2, 100)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "print mlp.grads(train_x[:,0:5], train_y[0:5])[0].shape\n",
    "print mlp.grads(train_x[:,0:5], train_y[0:5])[1].shape\n",
    "print mlp.grads(train_x[:,0:5], train_y[0:5])[2].shape\n",
    "print mlp.grads(train_x[:,0:5], train_y[0:5])[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64/64 (100%)   Epoch  1/ 5 in 23.08 seg\n",
      "Batch 64/64 (100%)   Epoch  2/ 5 in 22.75 seg\n",
      "Batch 64/64 (100%)   Epoch  3/ 5 in 22.81 seg\n",
      "Batch 64/64 (100%)   Epoch  4/ 5 in 23.33 seg\n",
      "Batch 64/64 (100%)   Epoch  5/ 5 in 23.40 seg\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "\n",
    "# Instantiate model\n",
    "mlp = dl.NumpyMLP(geometry, actvfunc) \n",
    "\n",
    "# Model parameters\n",
    "n_iter = 5\n",
    "bsize  = 25\n",
    "lrate  = 0.01\n",
    "\n",
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Architecture: ([13989, 100, 2])\n",
      "Amazon Sentiment Accuracy train: 0.850000 test: 0.737500\n"
     ]
    }
   ],
   "source": [
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "\n",
    "print \"MLP Architecture: (%s)\" % (geometry)\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\" % ( acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2 \n",
    "#### Begin Exercise 5.2\n",
    "\n",
    "Get in contact with Theano. Learn the difference between a symbolic\n",
    "representation and a function. Start by implementing the first layer of our\n",
    "previous MLP in Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy code\n",
    "x        = test_x             # Test set \n",
    "W1, b1   = mlp.params[:2]     # Weights and bias of fist layer \n",
    "z1       = np.dot(W1, x) + b1 # Linear transformation\n",
    "tilde_z1 = 1/(1+np.exp(-z1))  # Non-linear transformation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano code. \n",
    "# NOTE: We use undescore to denote symbolic equivalents to Numpy variables. \n",
    "# This is no Python convention!.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "_x = T.matrix('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this variable does not have any particular value, nor a space\n",
    "reserved in memory for it. It contains just a symbolic definition of what the\n",
    "variable can contain. The particular values will be given when we use it to\n",
    "compile a function. \n",
    "\n",
    "We could actually use the same definition format to define the weights and give\n",
    "their particular values as inputs to the compiled function. However, since we\n",
    "will be using a more complicated format in later exercises, we will use it here\n",
    "as well. The **```shared```** class allows to define variables that are shared\n",
    "across functions. They are also given a concrete value so that we do not need\n",
    "to give it for each function call. This format is therefore ideal for the\n",
    "weights of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets describe the operations we want to do with the variables. Again only\n",
    "symbolically. This is done by replacing our usual operations by Theano symbolic\n",
    "ones when necessary e. g. the internal product dot() or the sigmoid. Some\n",
    "operations like e.g. $+$ are automatically recognized by Theano (operator\n",
    "overloading). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z1            = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1      = T.nnet.sigmoid(_z1)\n",
    "# Keep in mind that naming variables is useful when debugging\n",
    "_z1.name       = 'z1'\n",
    "_tilde_z1.name = 'tilde_z1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging the code it is often useful to print the graph of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid [id A] 'tilde_z1'   \n",
      " |Elemwise{add,no_inplace} [id B] 'z1'   \n",
      "   |dot [id C] ''   \n",
      "   | |W1 [id D]\n",
      "   | |x [id E]\n",
      "   |b1 [id F]\n"
     ]
    }
   ],
   "source": [
    "# Perceptron computation graph\n",
    "theano.printing.debugprint(_tilde_z1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep in mind that, until this point, we do not have a\n",
    "function we can use to produce any practical input. In order to obtain this we\n",
    "have to compile this function by calling    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1 = theano.function([_x], _tilde_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of [ ] for the input variables, even if we just specify one\n",
    "variable. We can now do a test to compare the Numpy and Theano implementations\n",
    "and see that they give the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numpy and Theano Perceptrons are equivalent\n"
     ]
    }
   ],
   "source": [
    "# Check Numpy and Theano match\n",
    "if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):\n",
    "    print \"\\nNumpy and Theano Perceptrons are equivalent\"\n",
    "else:\n",
    "    raise ValueError, \"Numpy and Theano Perceptrons are different\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End exercise 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic forward pass\n",
    "In the previous section you have seen how to create symbolic Theano functions\n",
    "with shared parameters. You have thus all you need to implement the whole\n",
    "forward pass of a generic MLP in Theano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.3\n",
    "\n",
    "#### Begin Exercise 5.3\n",
    "\n",
    "Complete the method **```_forward()```** inside of the **```lxmls/deep\\_learning/mlp.py```**, in the class TheanoMLP.\n",
    "\n",
    "Note that this is called only once at the initialization of the\n",
    "class. To debug your implementation put a breakpoint at the \\_\\_init\\_\\_\n",
    "function call. Hint: Note that this is very similar to NumpyMLP.forward().\n",
    "You just need to keep track of the symbolic variable representing the output of\n",
    "the network after each layer is applied and compile the function at the end.\n",
    "After you are finished instantiate a Theano class and check that Numpy and\n",
    "Theano forward pass are the same. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning import mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_a = mlp.NumpyMLP(geometry, actvfunc)\n",
    "mlp_b = mlp.TheanoMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Exercise 5.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic differentiation\n",
    "In the previous section we compiled the forward pass of a MLP. In this section\n",
    "we will do the same with the cost used for training. We will also derive the\n",
    "gradients although this will be trivial once we have the cost function compiled.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4\n",
    "\n",
    "We first see an example that does not use any of the code in TheanoMLP but\n",
    "rather continues from what you wrote in exercise 6.3. In this exercise you\n",
    "completed a sigmoid layer with Theano. To get some values for the weights we\n",
    "used the first layer of the network you trained in 6.2. now we are going to use\n",
    "the second layer as well. This is thus assuming that your network in 6.2 has\n",
    "only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the\n",
    "case before starting this exercise.  \n",
    "\n",
    "For the sake of clarity, lets write here the part of Ex. 5.2 that we had completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the values from our MLP from Ex 6.2\n",
    "W1, b1   = mlp_a.params[:2]     # Weights and bias of fist layer \n",
    "# First layer symbolic variables\n",
    "_x  = T.matrix('x')\n",
    "_W1 = theano.shared(value=W1, name='W1', borrow=True) \n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) \n",
    "# First layer symbolic expressions\n",
    "_z1       = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1 = T.nnet.sigmoid(_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: If you don't want gradients..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional:  Feedback aligment\n",
    "\n",
    "The following class is build to learn using direct feedback aligment. This is not in the guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "\n",
    "\n",
    "def index2onehot(index, N):\n",
    "    \"\"\"\n",
    "    Transforms index to one-hot representation, for example\n",
    "\n",
    "    Input: e.g. index = [1, 2, 0], N = 4\n",
    "    Output:     [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0]]\n",
    "    \"\"\"\n",
    "    L = index.shape[0]\n",
    "    onehot = np.zeros((N, L))\n",
    "    for l in np.arange(L):\n",
    "        onehot[index[l], l] = 1\n",
    "    return onehot\n",
    "\n",
    "class NumpyMLP_nobackprop:\n",
    "    \"\"\"\n",
    "    Basic MLP with forward-pass and non_gradient computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, geometry, actvfunc, rng=None, model_file=None):\n",
    "        \"\"\"\n",
    "        Input: geometry  tuple with sizes of layer\n",
    "\n",
    "        Input: actvfunc  list of strings indicating the type of activation\n",
    "                         function. Supported 'sigmoid', 'softmax'\n",
    "\n",
    "        Input: rng       string inidcating random seed\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate random seed if not provided\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "\n",
    "        # CHECK THE PARAMETERS ARE VALID\n",
    "        self.sanity_checks(geometry, actvfunc)\n",
    "\n",
    "        # THIS DEFINES THE MLP\n",
    "        self.n_layers = len(geometry) - 1\n",
    "        if model_file:\n",
    "            if geometry or actvfunc:\n",
    "                raise ValueError(\"If you load a model geometry and actvfunc\"\n",
    "                                 \"should be None\")\n",
    "            self.params, self.actvfunc = self.load(model_file)\n",
    "        else:\n",
    "            # Parameters are stored as [weight0, bias0, weight1, bias1, ... ]\n",
    "            # for consistency with the theano way of storing parameters\n",
    "            self.params = self.init_weights(rng, geometry, actvfunc)\n",
    "            self.actvfunc = actvfunc\n",
    "\n",
    "    def forward(self, x, all_outputs=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        all_outputs = True  return intermediate activations\n",
    "        \"\"\"\n",
    "\n",
    "        # This will store activations at each layer and the input. This is\n",
    "        # needed to compute backpropagation\n",
    "        if all_outputs:\n",
    "            activations = []\n",
    "\n",
    "            # Input\n",
    "        tilde_z = x\n",
    "\n",
    "        for n in range(self.n_layers):\n",
    "\n",
    "            # Get weigths and bias of the layer (even and odd positions)\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "\n",
    "            # Linear transformation\n",
    "            z = np.dot(W, tilde_z) + b\n",
    "\n",
    "            # Non-linear transformation\n",
    "            if self.actvfunc[n] == \"sigmoid\":\n",
    "                tilde_z = 1.0 / (1+np.exp(-z))\n",
    "\n",
    "            elif self.actvfunc[n] == \"softmax\":\n",
    "                # Softmax is computed in log-domain to prevent\n",
    "                # underflow/overflow\n",
    "                tilde_z = np.exp(z - logsumexp(z, 0))\n",
    "\n",
    "            if all_outputs:\n",
    "                activations.append(tilde_z)\n",
    "\n",
    "        if all_outputs:\n",
    "            tilde_z = activations\n",
    "\n",
    "        return tilde_z\n",
    "        \n",
    "\n",
    "    def grads(self, x, y):\n",
    "        \"\"\"\n",
    "       Computes the gradients of the network with respect to cross entropy\n",
    "       error cost\n",
    "       \"\"\"\n",
    "\n",
    "        # Run forward and store activations for each layer\n",
    "        activations = self.forward(x, all_outputs=True)\n",
    "\n",
    "        # For each layer in reverse store the gradients for each parameter\n",
    "        nabla_params = [None] * (2*self.n_layers)\n",
    "\n",
    "        for n in np.arange(self.n_layers-1, -1, -1):\n",
    "            # Get weigths and bias (always in even and odd positions)\n",
    "            # Note that sometimes we need the weight from the next layer\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "            if n != self.n_layers-1:\n",
    "                F_mat = self.feedback_matrix[2*(n+1)]\n",
    "\n",
    "            if n == self.n_layers-1:\n",
    "                # NOTE: This assumes cross entropy cost\n",
    "                if self.actvfunc[n] == 'sigmoid':\n",
    "                    e = (activations[n]-y) / y.shape[0]\n",
    "                elif self.actvfunc[n] == 'softmax':\n",
    "                    I = index2onehot(y, W.shape[0])\n",
    "                    e = (activations[n]-I) / y.shape[0]\n",
    "            else:\n",
    "                e = np.dot(F_mat, e)\n",
    "                # This is correct but confusing n+1 is n in the guide\n",
    "                e *= activations[n] * (1-activations[n])\n",
    "\n",
    "            # Weight gradient\n",
    "            nabla_W = np.zeros(W.shape)\n",
    "            for l in np.arange(e.shape[1]):\n",
    "                if n == 0:\n",
    "                    # For the first layer, the activation is the input\n",
    "                    nabla_W += np.outer(e[:, l], x[:, l])\n",
    "                else:\n",
    "                    nabla_W += np.outer(e[:, l], activations[n-1][:, l])\n",
    "            # Bias gradient\n",
    "            nabla_b = np.sum(e, 1, keepdims=True)\n",
    "\n",
    "            # Store the gradients\n",
    "            nabla_params[2*n] = nabla_W\n",
    "            nabla_params[2*n+1] = nabla_b\n",
    "\n",
    "        return nabla_params\n",
    "\n",
    "    def init_weights(self, rng, geometry, actvfunc):\n",
    "        \"\"\"\n",
    "       Following theano tutorial at\n",
    "       http://deeplearning.net/software/theano/tutorial/\n",
    "       \"\"\"\n",
    "        params = []\n",
    "        self.feedback_matrix = []\n",
    "        for n in range(self.n_layers):\n",
    "            n_in, n_out = geometry[n:n+2]\n",
    "            weight = rng.uniform(low=-np.sqrt(6./(n_in+n_out)),\n",
    "                                 high=np.sqrt(6./(n_in+n_out)),\n",
    "                                 size=(n_out, n_in))\n",
    "            if actvfunc[n] == 'sigmoid':\n",
    "                weight *= 4\n",
    "            elif actvfunc[n] == 'softmax':\n",
    "                weight *= 4\n",
    "            bias = np.zeros((n_out, 1))\n",
    "            # Append parameters\n",
    "            params.append(weight)\n",
    "            params.append(bias)\n",
    "            self.feedback_matrix.append( np.random.uniform(-0.5, 0.5, weight.T.shape) )\n",
    "            self.feedback_matrix.append( np.random.uniform(-0.5, 0.5, bias.shape) )\n",
    "\n",
    "        return params\n",
    "\n",
    "    def sanity_checks(self, geometry, actvfunc):\n",
    "        # CHECK ACTIVATIONS\n",
    "        if actvfunc:\n",
    "            # Supported actvfunc\n",
    "            supported_acts = ['sigmoid', 'softmax']\n",
    "            if geometry and (len(actvfunc) != len(geometry)-1):\n",
    "                raise ValueError(\"The number of layers and actvfunc does not match\")\n",
    "            elif any([act not in supported_acts for act in actvfunc]):\n",
    "                raise ValueError(\"Only these actvfunc supported %s\" % (\" \".join(supported_acts)))\n",
    "            # All internal layers must be a sigmoid\n",
    "            for internal_act in actvfunc[:-1]:\n",
    "                if internal_act != 'sigmoid':\n",
    "                    raise ValueError(\"Intermediate layers must be sigmoid\")\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"\n",
    "        Save model\n",
    "        \"\"\"\n",
    "        par = self.params + self.actvfunc\n",
    "        with open(model_path, 'wb') as fid:\n",
    "            cPickle.dump(par, fid, cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        \"\"\"\n",
    "        with open(model_path) as fid:\n",
    "            par = cPickle.load(fid, cPickle.HIGHEST_PROTOCOL)\n",
    "            params = par[:len(par)//2]\n",
    "            actvfunc = par[len(par)//2:]\n",
    "        return params, actvfunc\n",
    "\n",
    "    def plot_weights(self, show=True, aspect='auto'):\n",
    "        \"\"\"\n",
    "       Plots the weights of the newtwork\n",
    "       \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        for n in range(self.n_layers):\n",
    "            # Get weights\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "\n",
    "            plt.subplot(2, self.n_layers, n+1)\n",
    "            plt.imshow(W, aspect=aspect, interpolation='nearest')\n",
    "            plt.title('Layer %d Weight' % n)\n",
    "            plt.colorbar()\n",
    "            plt.subplot(2, self.n_layers, self.n_layers+(n+1))\n",
    "            plt.plot(b)\n",
    "            plt.title('Layer %d Bias' % n)\n",
    "            plt.colorbar()\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "\n",
    "# Instantiate model\n",
    "mlp = NumpyMLP_nobackprop(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Architecture: ([13989, 100, 2])\n",
      "Amazon Sentiment Accuracy train: 0.542500 test: 0.532500\n"
     ]
    }
   ],
   "source": [
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "\n",
    "print \"MLP Architecture: (%s)\" % (geometry)\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\" % ( acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64/64 (100%)   Epoch  1/ 5 in 21.83 seg\n",
      "Batch 64/64 (100%)   Epoch  2/ 5 in 22.02 seg\n",
      "Batch 64/64 (100%)   Epoch  3/ 5 in 21.32 seg\n",
      "Batch 64/64 (100%)   Epoch  4/ 5 in 21.69 seg\n",
      "Batch 64/64 (100%)   Epoch  5/ 5 in 21.54 seg\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter=5, bsize=bsize, lrate=0.5, train_set=(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Architecture: ([13989, 100, 2])\n",
      "Amazon Sentiment Accuracy train: 0.911875 test: 0.770000\n"
     ]
    }
   ],
   "source": [
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "\n",
    "print \"MLP Architecture: (%s)\" % (geometry)\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\" % ( acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional:  Direct feedback aligment\n",
    "\n",
    "We can actually update the weights in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import logsumexp\n",
    "\n",
    "def index2onehot(index, N):\n",
    "    \"\"\"\n",
    "    Transforms index to one-hot representation, for example\n",
    "\n",
    "    Input: e.g. index = [1, 2, 0], N = 4\n",
    "    Output:     [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0]]\n",
    "    \"\"\"\n",
    "    L = index.shape[0]\n",
    "    onehot = np.zeros((N, L))\n",
    "    for l in np.arange(L):\n",
    "        onehot[index[l], l] = 1\n",
    "    return onehot\n",
    "\n",
    "class NumpyMLP_dfa:\n",
    "    \"\"\"\n",
    "    Basic MLP with forward-pass and non_gradient computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, geometry, actvfunc, rng=None, model_file=None):\n",
    "        \"\"\"\n",
    "        Input: geometry  tuple with sizes of layer\n",
    "\n",
    "        Input: actvfunc  list of strings indicating the type of activation\n",
    "                         function. Supported 'sigmoid', 'softmax'\n",
    "\n",
    "        Input: rng       string inidcating random seed\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate random seed if not provided\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "\n",
    "        # CHECK THE PARAMETERS ARE VALID\n",
    "        self.sanity_checks(geometry, actvfunc)\n",
    "\n",
    "        # THIS DEFINES THE MLP\n",
    "        self.n_layers = len(geometry) - 1\n",
    "        if model_file:\n",
    "            if geometry or actvfunc:\n",
    "                raise ValueError(\"If you load a model geometry and actvfunc\"\n",
    "                                 \"should be None\")\n",
    "            self.params, self.actvfunc = self.load(model_file)\n",
    "        else:\n",
    "            # Parameters are stored as [weight0, bias0, weight1, bias1, ... ]\n",
    "            # for consistency with the theano way of storing parameters\n",
    "            self.params = self.init_weights(rng, geometry, actvfunc)\n",
    "            self.actvfunc = actvfunc\n",
    "\n",
    "    def forward(self, x, all_outputs=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        all_outputs = True  return intermediate activations\n",
    "        \"\"\"\n",
    "\n",
    "        # This will store activations at each layer and the input. This is\n",
    "        # needed to compute backpropagation\n",
    "        if all_outputs:\n",
    "            activations = []\n",
    "\n",
    "            # Input\n",
    "        tilde_z = x\n",
    "\n",
    "        for n in range(self.n_layers):\n",
    "\n",
    "            # Get weigths and bias of the layer (even and odd positions)\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "\n",
    "            # Linear transformation\n",
    "            z = np.dot(W, tilde_z) + b\n",
    "\n",
    "            # Non-linear transformation\n",
    "            if self.actvfunc[n] == \"sigmoid\":\n",
    "                tilde_z = 1.0 / (1+np.exp(-z))\n",
    "\n",
    "            elif self.actvfunc[n] == \"softmax\":\n",
    "                # Softmax is computed in log-domain to prevent\n",
    "                # underflow/overflow\n",
    "                tilde_z = np.exp(z - logsumexp(z, 0))\n",
    "\n",
    "            if all_outputs:\n",
    "                activations.append(tilde_z)\n",
    "\n",
    "        if all_outputs:\n",
    "            tilde_z = activations\n",
    "\n",
    "        return tilde_z\n",
    "        \n",
    "\n",
    "    def grads(self, x, y):\n",
    "        \"\"\"\n",
    "       Computes the gradients of the network with respect to cross entropy\n",
    "       error cost\n",
    "       \"\"\"\n",
    "\n",
    "        # Run forward and store activations for each layer\n",
    "        activations = self.forward(x, all_outputs=True)\n",
    "\n",
    "        # For each layer in reverse store the gradients for each parameter\n",
    "        nabla_params = [None] * (2*self.n_layers)\n",
    "\n",
    "        for n in np.arange(self.n_layers-1, -1, -1):\n",
    "\n",
    "            # Get weigths and bias (always in even and odd positions)\n",
    "            # Note that sometimes we need the weight from the next layer\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "            if n != self.n_layers-1:\n",
    "                F_mat = self.feedback_matrix[2*(n+1)]\n",
    "\n",
    "            # NOTE: This assumes cross entropy cost\n",
    "            if self.actvfunc[n] == 'sigmoid':\n",
    "                e = (activations[n]-y) / y.shape[0]\n",
    "            elif self.actvfunc[n] == 'softmax':\n",
    "                I = index2onehot(y, W.shape[0])\n",
    "                e = (activations[n]-I) / y.shape[0]\n",
    "\n",
    "\n",
    "            # Weight gradient\n",
    "            nabla_W = np.zeros(W.shape)\n",
    "            for l in np.arange(e.shape[1]):\n",
    "                if n == 0:\n",
    "                    # For the first layer, the activation is the input\n",
    "                    nabla_W += np.outer(e[:, l], x[:, l])\n",
    "                else:\n",
    "                    nabla_W += np.outer(e[:, l], activations[n-1][:, l])\n",
    "            # Bias gradient\n",
    "            nabla_b = np.sum(e, 1, keepdims=True)\n",
    "\n",
    "            # Store the gradients\n",
    "            nabla_params[2*n] = nabla_W\n",
    "            nabla_params[2*n+1] = nabla_b\n",
    "\n",
    "        return nabla_params\n",
    "\n",
    "    def init_weights(self, rng, geometry, actvfunc):\n",
    "        \"\"\"\n",
    "        Following theano tutorial at\n",
    "        http://deeplearning.net/software/theano/tutorial/\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        self.feedback_matrix = []\n",
    "        n_classes = geometry[-1]\n",
    "        \n",
    "        for n in range(self.n_layers):\n",
    "            n_in, n_out = geometry[n:n+2]\n",
    "            weight = rng.uniform(low=-np.sqrt(6./(n_in+n_out)),\n",
    "                                 high=np.sqrt(6./(n_in+n_out)),\n",
    "                                 size=(n_out, n_in))\n",
    "            if actvfunc[n] == 'sigmoid':\n",
    "                weight *= 4\n",
    "            elif actvfunc[n] == 'softmax':\n",
    "                weight *= 4\n",
    "            bias = np.zeros((n_out, 1))\n",
    "            # Append parameters\n",
    "            params.append(weight)\n",
    "            params.append(bias)\n",
    "            self.feedback_matrix.append( np.random.uniform(-0.5, 0.5, (n_classes, weight.T.shape[1]) ) )\n",
    "            self.feedback_matrix.append( np.random.uniform(-0.5, 0.5, bias.shape) )\n",
    "\n",
    "        return params\n",
    "\n",
    "    def sanity_checks(self, geometry, actvfunc):\n",
    "\n",
    "        # CHECK ACTIVATIONS\n",
    "        if actvfunc:\n",
    "            # Supported actvfunc\n",
    "            supported_acts = ['sigmoid', 'softmax']\n",
    "            if geometry and (len(actvfunc) != len(geometry)-1):\n",
    "                raise ValueError(\"The number of layers and actvfunc does not match\")\n",
    "            elif any([act not in supported_acts for act in actvfunc]):\n",
    "                raise ValueError(\"Only these actvfunc supported %s\" % (\" \".join(supported_acts)))\n",
    "            # All internal layers must be a sigmoid\n",
    "            for internal_act in actvfunc[:-1]:\n",
    "                if internal_act != 'sigmoid':\n",
    "                    raise ValueError(\"Intermediate layers must be sigmoid\")\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"\n",
    "        Save model\n",
    "        \"\"\"\n",
    "        par = self.params + self.actvfunc\n",
    "        with open(model_path, 'wb') as fid:\n",
    "            cPickle.dump(par, fid, cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        \"\"\"\n",
    "        with open(model_path) as fid:\n",
    "            par = cPickle.load(fid, cPickle.HIGHEST_PROTOCOL)\n",
    "            params = par[:len(par)//2]\n",
    "            actvfunc = par[len(par)//2:]\n",
    "        return params, actvfunc\n",
    "\n",
    "    def plot_weights(self, show=True, aspect='auto'):\n",
    "        \"\"\"\n",
    "       Plots the weights of the newtwork\n",
    "       \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        for n in range(self.n_layers):\n",
    "            # Get weights\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "\n",
    "            plt.subplot(2, self.n_layers, n+1)\n",
    "            plt.imshow(W, aspect=aspect, interpolation='nearest')\n",
    "            plt.title('Layer %d Weight' % n)\n",
    "            plt.colorbar()\n",
    "            plt.subplot(2, self.n_layers, self.n_layers+(n+1))\n",
    "            plt.plot(b)\n",
    "            plt.title('Layer %d Bias' % n)\n",
    "            plt.colorbar()\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 100, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "\n",
    "# Instantiate model\n",
    "mlp =  NumpyMLP_dfa(geometry, actvfunc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Architecture: ([13989, 100, 2])\n",
      "Amazon Sentiment Accuracy train: 0.542500 test: 0.532500\n"
     ]
    }
   ],
   "source": [
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "\n",
    "print \"MLP Architecture: (%s)\" % (geometry)\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\" % ( acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64/64 (100%)   Epoch  1/ 5 in 23.40 seg\n",
      "Batch 64/64 (100%)   Epoch  2/ 5 in 22.85 seg\n",
      "Batch 64/64 (100%)   Epoch  3/ 5 in 22.64 seg\n",
      "Batch 64/64 (100%)   Epoch  4/ 5 in 22.77 seg\n",
      "Batch 64/64 (100%)   Epoch  5/ 5 in 23.43 seg\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter=5, bsize=bsize, lrate=0.5, train_set=(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Architecture: ([13989, 100, 2])\n",
      "Amazon Sentiment Accuracy train: 0.996250 test: 0.840000\n"
     ]
    }
   ],
   "source": [
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "\n",
    "print \"MLP Architecture: (%s)\" % (geometry)\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\" % ( acc_train, acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
