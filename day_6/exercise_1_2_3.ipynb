{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning 2\n",
    "\n",
    "\n",
    "| Exercise | Requires coding | What to code (if anything) | What should you learn (or practice)    | Done it! |\n",
    "|----------|-----------------|----------------------------|----------------------------------------|----------|\n",
    "| 6.1      | No             |                            | Understand RNN as Feedforward unfolded                     | No       |\n",
    "| 6.2      | Yes              |                            |Understand scan operation theano| No       |\n",
    "| 6.3      |   Yes         |   forward pass RNN theano                            | RNN                                | No       |\n",
    "| 6.4      |   No         |        | test pre-trained embeddings                           | No       |\n",
    "| 6.5      |   No         |        |  Understand LSTMs                               | No       |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../lxmls-toolkit')\n",
    "import lxmls\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1\n",
    "\n",
    "**Convince yourself that a RNN is just an FF unfolded in time. Run the NumpyRNN code. Set break-points and compare with what you learned about backpropagation in the previous day.**\n",
    "\n",
    "**To work with RNNs we will use the Part-of-speech data-set seen in the sequence models day.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus =  lxmls.readers.pos_corpus.PostagCorpus()\n",
    "\n",
    "data_path = \"../../lxmls-toolkit/data/\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"train-02-21.conll\",\n",
    "                                            max_sent_len=15,\n",
    "                                            max_nr_sent=1000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"test-23.conll\",\n",
    "                                           max_sent_len=15,\n",
    "                                           max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"dev-22.conll\", \n",
    "                                          max_sent_len=15,\n",
    "                                          max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_words: 19217\n",
      "nr_tags: 12\n"
     ]
    }
   ],
   "source": [
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)\n",
    "\n",
    "print \"nr_words:\", nr_words\n",
    "print \"nr_tags:\", nr_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19217"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 6, 0, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40, 43, 44, 41]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to redo the indices of the data so that they are consecutive and cast all data to numpy arrays of int32 for compatibility with GPUs. This function will also add reverse indices to recover tag and word from its index word dict and tag dict\n",
    "\n",
    "- **Why do the pcc.compacify changes the indicies of the words in train_seq, test_seq and dev_seq ?**\n",
    "\n",
    "    - the idea is the new mapping is compact: does not have unused indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Redo indices\n",
    "train_seq, test_seq, dev_seq = pcc.compacify(train_seq, test_seq, dev_seq, theano=True) # Get number of words and tags in the corpus\n",
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_words: 4786\n",
      "nr_tags: 12\n"
     ]
    }
   ],
   "source": [
    "print \"nr_words:\", nr_words\n",
    "print \"nr_tags:\", nr_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5, 536, 537, 165, 347, 105, 538, 289, 131, 539, 289,   4], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[100].x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and configure the NumpyRNN. Remember to user reload if you want to modify the code inside the rnns module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.deep_learning.rnn as rnns\n",
    "reload(rnns)\n",
    "# RNN configuration\n",
    "SEED = 1234 # Random seed to initialize weigths\n",
    "emb_size = 50 # Size of word embeddings\n",
    "hidden_size = 20 # size of hidden layer\n",
    "\n",
    "np_rnn = rnns.NumpyRNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)\n",
    "x0 = train_seq[0].x\n",
    "y0 = train_seq[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "p_y, y_rnn, h, z1, x = np_rnn.forward(x0, all_outputs=True) \n",
    "\n",
    "# Gradients\n",
    "numpy_rnn_gradients = np_rnn.grads(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan operation in theano\n",
    "\n",
    "Handling variable length computation graphs in an automatic fashion is not simple. Theano provides the scan function for this purpose. The scan function acts as a symbolic “for” loop. Since, unlike for normal python “for” loops, it is not possible to put a breakpoint in the scan loop, the design of graphs with scan has to be handled with care. Toolboxes like Keras conveniently abstract the user from such constructs. However, for complex designs it will be necessary to be able to use scan or equivalent functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "\n",
    "**Understand the basics of scan with these examples. Scan allows you to build computation graphs with a variable number of nodes. It acts as a python ”for” loop but it is symbolic. The following example should help you understand the basic scan functionality. It generates a sequence for a given length. Run it and modify it. Try to arrive at an error and understand what happened.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "theano.config.optimizer='None'\n",
    "\n",
    "def square(x): \n",
    "    return x**2\n",
    "\n",
    "# Python\n",
    "def np_square_n_steps(nr_steps): \n",
    "    out = []\n",
    "    for n in np.arange(nr_steps): \n",
    "        out.append(square(n))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36 49 64 81]\n",
      "[ 0  1  4  9 16 25 36 49 64 81]\n"
     ]
    }
   ],
   "source": [
    "# Theano\n",
    "nr_steps = T.lscalar('nr_steps')\n",
    "h, _ = theano.scan(fn=square, sequences=T.arange(nr_steps))\n",
    "th_square_n_steps = theano.function([nr_steps], h)\n",
    "\n",
    "# Compare both\n",
    "print np_square_n_steps(10)\n",
    "print th_square_n_steps(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example should help you understand about matrix multiplications and passing values from one iteration to the other. At each step, we will multiply the output of the previous step by a matrix A. We start with an initial vector s0. The matrix and vector are random but normalized to result on a Markov chain (this is irrelevant for the use of scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "nr_states = 3\n",
    "nr_steps = 5\n",
    "\n",
    "# Transition matrix\n",
    "A = np.abs(np.random.randn(nr_states, nr_states)) \n",
    "A = A/A.sum(0, keepdims=True)\n",
    "\n",
    "# Initial state\n",
    "s0 = np.zeros(nr_states)\n",
    "s0[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy version\n",
    "def np_markov_step(s_tm1): \n",
    "    s_t = np.dot(s_tm1, A.T) \n",
    "    return s_t\n",
    "\n",
    "def np_markov_chain(nr_steps, A, s0):\n",
    "    # Pre-allocate space\n",
    "    s = np.zeros((nr_steps+1, nr_states)) \n",
    "    s[0, :] = s0\n",
    "    for t in np.arange(nr_steps):\n",
    "         s[t+1, :] = np_markov_step(s[t, :]) \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.36331926,  0.00998989,  0.62669084],\n",
       "       [ 0.22093512,  0.07267485,  0.70639002],\n",
       "       [ 0.22723631,  0.08820146,  0.68456223],\n",
       "       [ 0.23850484,  0.08797088,  0.67352428],\n",
       "       [ 0.24099095,  0.08686008,  0.67214897]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_markov_chain(nr_steps, A, s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.36331926,  0.00998989,  0.62669084],\n",
       "       [ 0.22093512,  0.07267485,  0.70639002],\n",
       "       [ 0.22723631,  0.08820146,  0.68456223],\n",
       "       [ 0.23850484,  0.08797088,  0.67352428],\n",
       "       [ 0.24099095,  0.08686008,  0.67214897]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Theano version\n",
    "# Store variables as shared variables\n",
    "th_A = theano.shared(A, name='A', borrow=True) \n",
    "th_s0 = theano.shared(s0, name='s0', borrow=True) \n",
    "\n",
    "# Symbolic variable for the number of steps \n",
    "th_nr_steps = T.lscalar('nr_steps')\n",
    "\n",
    "def th_markov_step(s_tm1):\n",
    "    s_t = T.dot(s_tm1, th_A.T)\n",
    "    # Remember to name variables \n",
    "    s_t.name = 's_t'\n",
    "    return s_t\n",
    "\n",
    "s, _ = theano.scan(th_markov_step, \n",
    "                   outputs_info=[dict(initial=th_s0)],\n",
    "                   n_steps=th_nr_steps)\n",
    "\n",
    "th_markov_chain = theano.function([th_nr_steps], T.concatenate((th_s0[None, :], s), 0))\n",
    "\n",
    "th_markov_chain(nr_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A RNN in Theano for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.3\n",
    "\n",
    "** Complete the theano code for a RNN inside lxmls/deep learning/rnn.py. Use exercise 6.1 for a numpy example and 6.2 to learn how to handle scan. Keep in mind that you only need to implement the forward pass! Theano will handle backpropagation for us.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "rnn = rnns.RNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)\n",
    "# Compile the forward pass function\n",
    "x = T.ivector('x')\n",
    "th_forward = theano.function([x], rnn._forward(x).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When working with theano, it is more difficult to localize the source of errors. It is therefore important to work step by step and test the code frequently. To debug we suggest to implement and compile the forward pass first. You can use this code for testing. If it raises no error you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scan in module theano.scan_module.scan:\n",
      "\n",
      "scan(fn, sequences=None, outputs_info=None, non_sequences=None, n_steps=None, truncate_gradient=-1, go_backwards=False, mode=None, name=None, profile=False, allow_gc=None, strict=False)\n",
      "    This function constructs and applies a Scan op to the provided\n",
      "    arguments.\n",
      "    \n",
      "    :param fn:\n",
      "        ``fn`` is a function that describes the operations involved in one\n",
      "        step of ``scan``. ``fn`` should construct variables describing the\n",
      "        output of one iteration step. It should expect as input theano\n",
      "        variables representing all the slices of the input sequences\n",
      "        and previous values of the outputs, as well as all other arguments\n",
      "        given to scan as ``non_sequences``. The order in which scan passes\n",
      "        these variables to ``fn``  is the following :\n",
      "    \n",
      "        * all time slices of the first sequence\n",
      "        * all time slices of the second sequence\n",
      "        * ...\n",
      "        * all time slices of the last sequence\n",
      "        * all past slices of the first output\n",
      "        * all past slices of the second otuput\n",
      "        * ...\n",
      "        * all past slices of the last output\n",
      "        * all other arguments (the list given as `non_sequences` to\n",
      "            scan)\n",
      "    \n",
      "        The order of the sequences is the same as the one in the list\n",
      "        `sequences` given to scan. The order of the outputs is the same\n",
      "        as the order of ``outputs_info``. For any sequence or output the\n",
      "        order of the time slices is the same as the one in which they have\n",
      "        been given as taps. For example if one writes the following :\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            scan(fn, sequences = [ dict(input= Sequence1, taps = [-3,2,-1])\n",
      "                                 , Sequence2\n",
      "                                 , dict(input =  Sequence3, taps = 3) ]\n",
      "                   , outputs_info = [ dict(initial =  Output1, taps = [-3,-5])\n",
      "                                    , dict(initial = Output2, taps = None)\n",
      "                                    , Output3 ]\n",
      "                   , non_sequences = [ Argument1, Argument2])\n",
      "    \n",
      "        ``fn`` should expect the following arguments in this given order:\n",
      "    \n",
      "        #. ``Sequence1[t-3]``\n",
      "        #. ``Sequence1[t+2]``\n",
      "        #. ``Sequence1[t-1]``\n",
      "        #. ``Sequence2[t]``\n",
      "        #. ``Sequence3[t+3]``\n",
      "        #. ``Output1[t-3]``\n",
      "        #. ``Output1[t-5]``\n",
      "        #. ``Output3[t-1]``\n",
      "        #. ``Argument1``\n",
      "        #. ``Argument2``\n",
      "    \n",
      "        The list of ``non_sequences`` can also contain shared variables\n",
      "        used in the function, though ``scan`` is able to figure those\n",
      "        out on its own so they can be skipped. For the clarity of the\n",
      "        code we recommend though to provide them to scan. To some extend\n",
      "        ``scan`` can also figure out other ``non sequences`` (not shared)\n",
      "        even if not passed to scan (but used by `fn`). A simple example of\n",
      "        this would be :\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            import theano.tensor as TT\n",
      "            W   = TT.matrix()\n",
      "            W_2 = W**2\n",
      "            def f(x):\n",
      "                return TT.dot(x,W_2)\n",
      "    \n",
      "        The function is expected to return two things. One is a list of\n",
      "        outputs ordered in the same order as ``outputs_info``, with the\n",
      "        difference that there should be only one output variable per\n",
      "        output initial state (even if no tap value is used). Secondly\n",
      "        `fn` should return an update dictionary (that tells how to\n",
      "        update any shared variable after each iteration step). The\n",
      "        dictionary can optionally be given as a list of tuples. There is\n",
      "        no constraint on the order of these two list, ``fn`` can return\n",
      "        either ``(outputs_list, update_dictionary)`` or\n",
      "        ``(update_dictionary, outputs_list)`` or just one of the two (in\n",
      "        case the other is empty).\n",
      "    \n",
      "        To use ``scan`` as a while loop, the user needs to change the\n",
      "        function ``fn`` such that also a stopping condition is returned.\n",
      "        To do so, he/she needs to wrap the condition in an ``until`` class.\n",
      "        The condition should be returned as a third element, for example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            ...\n",
      "            return [y1_t, y2_t], {x:x+1}, theano.scan_module.until(x < 50)\n",
      "    \n",
      "        Note that a number of steps (considered in here as the maximum\n",
      "        number of steps ) is still required even though a condition is\n",
      "        passed (and it is used to allocate memory if needed). = {}):\n",
      "    \n",
      "    :param sequences:\n",
      "        ``sequences`` is the list of Theano variables or dictionaries\n",
      "        describing the sequences ``scan`` has to iterate over. If a\n",
      "        sequence is given as wrapped in a dictionary, then a set of optional\n",
      "        information can be provided about the sequence. The dictionary\n",
      "        should have the following keys:\n",
      "    \n",
      "        * ``input`` (*mandatory*) -- Theano variable representing the\n",
      "          sequence.\n",
      "    \n",
      "        * ``taps`` -- Temporal taps of the sequence required by ``fn``.\n",
      "          They are provided as a list of integers, where a value ``k``\n",
      "          impiles that at iteration step ``t`` scan will pass to ``fn``\n",
      "          the slice ``t+k``. Default value is ``[0]``\n",
      "    \n",
      "        Any Theano variable in the list ``sequences`` is automatically\n",
      "        wrapped into a dictionary where ``taps`` is set to ``[0]``\n",
      "    \n",
      "    \n",
      "    :param outputs_info:\n",
      "        ``outputs_info`` is the list of Theano variables or dictionaries\n",
      "        describing the initial state of the outputs computed\n",
      "        recurrently. When this initial states are given as dictionary\n",
      "        optional information can be provided about the output corresponding\n",
      "        to these initial states. The dictionary should have the following\n",
      "        keys:\n",
      "    \n",
      "        * ``initial`` -- Theano variable that represents the initial\n",
      "          state of a given output. In case the output is not computed\n",
      "          recursively (think of a map) and does not require an initial\n",
      "          state this field can be skipped. Given that (only) the previous\n",
      "          time step of the output is used by ``fn``, the initial state\n",
      "          **should have the same shape** as the output and **should not\n",
      "          involve a downcast** of the data type of the output. If multiple\n",
      "          time taps are used, the initial state should have one extra\n",
      "          dimension that should cover all the possible taps. For example\n",
      "          if we use ``-5``, ``-2`` and ``-1`` as past taps, at step 0,\n",
      "          ``fn`` will require (by an abuse of notation) ``output[-5]``,\n",
      "          ``output[-2]`` and ``output[-1]``. This will be given by\n",
      "          the initial state, which in this case should have the shape\n",
      "          (5,)+output.shape. If this variable containing the initial\n",
      "          state is called ``init_y`` then ``init_y[0]`` *corresponds to*\n",
      "          ``output[-5]``. ``init_y[1]`` *correponds to* ``output[-4]``,\n",
      "          ``init_y[2]`` corresponds to ``output[-3]``, ``init_y[3]``\n",
      "          coresponds to ``output[-2]``, ``init_y[4]`` corresponds to\n",
      "          ``output[-1]``. While this order might seem strange, it comes\n",
      "          natural from splitting an array at a given point. Assume that\n",
      "          we have a array ``x``, and we choose ``k`` to be time step\n",
      "          ``0``. Then our initial state would be ``x[:k]``, while the\n",
      "          output will be ``x[k:]``. Looking at this split, elements in\n",
      "          ``x[:k]`` are ordered exactly like those in ``init_y``.\n",
      "        * ``taps`` -- Temporal taps of the output that will be pass to\n",
      "          ``fn``. They are provided as a list of *negative* integers,\n",
      "          where a value ``k`` implies that at iteration step ``t`` scan\n",
      "          will pass to ``fn`` the slice ``t+k``.\n",
      "    \n",
      "        ``scan`` will follow this logic if partial information is given:\n",
      "    \n",
      "        * If an output is not wrapped in a dictionary, ``scan`` will wrap\n",
      "          it in one assuming that you use only the last step of the output\n",
      "          (i.e. it makes your tap value list equal to [-1]).\n",
      "        * If you wrap an output in a dictionary and you do not provide any\n",
      "          taps but you provide an initial state it will assume that you are\n",
      "          using only a tap value of -1.\n",
      "        * If you wrap an output in a dictionary but you do not provide any\n",
      "          initial state, it assumes that you are not using any form of\n",
      "          taps.\n",
      "        * If you provide a ``None`` instead of a variable or a empty\n",
      "          dictionary ``scan`` assumes that you will not use any taps for\n",
      "          this output (like for example in case of a map)\n",
      "    \n",
      "        If ``outputs_info`` is an empty list or None, ``scan`` assumes\n",
      "        that no tap is used for any of the outputs. If information is\n",
      "        provided just for a subset of the outputs an exception is\n",
      "        raised (because there is no convention on how scan should map\n",
      "        the provided information to the outputs of ``fn``)\n",
      "    \n",
      "    \n",
      "    :param non_sequences:\n",
      "        ``non_sequences`` is the list of arguments that are passed to\n",
      "        ``fn`` at each steps. One can opt to exclude variable\n",
      "        used in ``fn`` from this list as long as they are part of the\n",
      "        computational graph, though for clarity we encourage not to do so.\n",
      "    \n",
      "    \n",
      "    :param n_steps:\n",
      "        ``n_steps`` is the number of steps to iterate given as an int\n",
      "        or Theano scalar. If any of the input sequences do not have\n",
      "        enough elements, scan will raise an error. If the *value is 0* the\n",
      "        outputs will have *0 rows*. If the value is negative, ``scan``\n",
      "        will run backwards in time. If the ``go_backwards`` flag is already\n",
      "        set and also ``n_steps`` is negative, ``scan`` will run forward\n",
      "        in time. If n_steps is not provided, ``scan`` will figure\n",
      "        out the amount of steps it should run given its input sequences.\n",
      "    \n",
      "    \n",
      "    :param truncate_gradient:\n",
      "        ``truncate_gradient`` is the number of steps to use in truncated\n",
      "        BPTT.  If you compute gradients through a scan op, they are\n",
      "        computed using backpropagation through time. By providing a\n",
      "        different value then -1, you choose to use truncated BPTT instead\n",
      "        of classical BPTT, where you go for only ``truncate_gradient``\n",
      "        number of steps back in time.\n",
      "    \n",
      "    \n",
      "    :param go_backwards:\n",
      "        ``go_backwards`` is a flag indicating if ``scan`` should go\n",
      "        backwards through the sequences. If you think of each sequence\n",
      "        as indexed by time, making this flag True would mean that\n",
      "        ``scan`` goes back in time, namely that for any sequence it\n",
      "        starts from the end and goes towards 0.\n",
      "    \n",
      "    \n",
      "    :param name:\n",
      "        When profiling ``scan``, it is crucial to provide a name for any\n",
      "        instance of ``scan``. The profiler will produce an overall\n",
      "        profile of your code as well as profiles for the computation of\n",
      "        one step of each instance of ``scan``. The ``name`` of the instance\n",
      "        appears in those profiles and can greatly help to disambiguate\n",
      "        information.\n",
      "    \n",
      "    :param mode:\n",
      "        It is recommended to leave this argument to None, especially\n",
      "        when profiling ``scan`` (otherwise the results are not going to\n",
      "        be accurate). If you prefer the computations of one step of\n",
      "        ``scan`` to be done differently then the entire function, you\n",
      "        can use this parameter to describe how the computations in this\n",
      "        loop are done (see ``theano.function`` for details about\n",
      "        possible values and their meaning).\n",
      "    \n",
      "    :param profile:\n",
      "        Flag or string. If true, or different from the empty string, a\n",
      "        profile object will be created and attached to the inner graph of\n",
      "        scan. In case ``profile`` is True, the profile object will have the\n",
      "        name of the scan instance, otherwise it will have the passed string.\n",
      "        Profile object collect (and print) information only when running the\n",
      "        inner graph with the new cvm linker ( with default modes,\n",
      "        other linkers this argument is useless)\n",
      "    \n",
      "    :param allow_gc:\n",
      "        Set the value of allow gc for the internal graph of scan.  If\n",
      "        set to None, this will use the value of config.scan.allow_gc.\n",
      "    \n",
      "    :param strict:\n",
      "        If true, all the shared variables used in ``fn`` must be provided as a\n",
      "        part of ``non_sequences`` or ``sequences``. \n",
      "    \n",
      "    :rtype: tuple\n",
      "    :return: tuple of the form (outputs, updates); ``outputs`` is either a\n",
      "             Theano variable or a list of Theano variables representing the\n",
      "             outputs of ``scan`` (in the same order as in\n",
      "             ``outputs_info``). ``updates`` is a subclass of dictionary\n",
      "             specifying the\n",
      "             update rules for all shared variables used in scan\n",
      "             This dictionary should be passed to ``theano.function`` when\n",
      "             you compile your function. The change compared to a normal\n",
      "             dictionary is that we validate that keys are SharedVariable\n",
      "             and addition of those dictionary are validated to be consistent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(theano.scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of Pre-training\n",
    "\n",
    "One of the key insights that has played a role in the rise of deep learning in NLP tasks is the use of neural word-embeddings. These are just numeric representations of words that can be learned from unsupervised data using simple FF networks such as e.g. skip-grams.\n",
    "\n",
    "Such representations can be plugged into supervised models such as the RNN that we just trained to ini- tialize its initial layer. The use of pre-trained embeddings very often leads to important improvements in performance.\n",
    "\n",
    "### Exercise 6.4 \n",
    "**Test the effect of using pre-trained embeddings. Run the following code to download the embeddings. Reset the layer parameters and initialize the embedding layer with the pre-trained embeddings. Then run the training code from the last exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  More Complex RNNs: LSTMs \n",
    "\n",
    "As previously mentioned, the basic RNN that we saw until now suffers from a number of drawbacks like exploding or vanishing gradients, and the inability to model long term dependencies. These deficiencies are solved by more complex RNNs that include more complex internal logics, and are able to selectively forget, thus capturing better long term information.\n",
    "\n",
    "The so called Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have replaced simple RNNs in most of modern models. However, they only imply more complex architectures. For an excellent visual explanation of the differences between simple RNNs, GRUs and LSTMs please visit\n",
    "\n",
    "### Exercise 6.5 \n",
    "\n",
    "\n",
    "**Convince yourself that LSTMs are just more complex RNNs. Run them, play around with the hyper parameters and compare the RNN and LSTM classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
